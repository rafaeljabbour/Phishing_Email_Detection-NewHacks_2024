{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "649eae4a-0260-452d-afde-0523fd8c6148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (4.12.3)\n",
      "Requirement already satisfied: tldextract in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (5.1.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: idna in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from tldextract) (3.8)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from tldextract) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from tldextract) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from tldextract) (3.16.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from requests>=2.1.0->tldextract) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from requests>=2.1.0->tldextract) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mateotomeho/Library/Python/3.11/lib/python/site-packages (from requests>=2.1.0->tldextract) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas beautifulsoup4 tldextract\n",
    "#Pandas = For data manipulation and analysis + data structures and reading data from files (CSV, JSON, TXT)\n",
    "#Beautifulsoup4 = Parsing HTML and XML documents to extract specific info from HTML content (Web scraping)\n",
    "#tldetract = Extract the top-level domain (TLD) from a URL, so break down URL into components and useful for analyzing and processing URL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "056661ac-3ece-4dd3-ad73-9d0018e37a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the librairies needed for the extracting functions\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import tldextract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5e50746d-f690-475e-b23c-42e8f266d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regex patterns for URL, email, IP, and href detection\n",
    "\n",
    "url_regex = r'https?://(?:www\\.)?[^\\s/$.?#].[^\\s]*'\n",
    "href_regex = r'href=[\"\\'](https?://[^\\s\"\\'<>]+)[\"\\']'\n",
    "ip_regex = r'(https?://)?\\b\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\b'\n",
    "email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "309a8eec-1ee8-4f96-b1ae-4c38749efbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the function helper for the HTML extraction\n",
    "\n",
    "#1) Check if there is a @ in the url (Bool)\n",
    "def has_at_in_urls(urls):\n",
    "    email_pattern = re.compile(email_regex, re.IGNORECASE)\n",
    "    #Store all the email pattern to detect if it is a emal addresses\n",
    "    for url in urls:\n",
    "        if \"@\" in url and not email_pattern.search(url):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def number_attachments(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Initialize the count of attachments\n",
    "    total_attachments = 0\n",
    "\n",
    "    # Count <a> tags that link to downloadable files (including images if needed)\n",
    "    total_attachments += len(soup.find_all('a', href=lambda href: href and href.endswith(('.pdf', '.doc', '.ppt', '.zip', '.txt', '.jpg', '.png', '.gif'))))\n",
    "\n",
    "    # Count <embed> tags (if they link to downloadable content)\n",
    "    total_attachments += len(soup.find_all('embed', src=lambda src: src and src.endswith(('.pdf', '.doc', '.ppt', '.zip', '.txt', '.jpg', '.png', '.gif'))))\n",
    "\n",
    "    # Count <object> tags (commonly used for embedding downloadable multimedia)\n",
    "    total_attachments += len(soup.find_all('object', data=lambda data: data and data.endswith(('.pdf', '.doc', '.ppt', '.zip', '.txt', '.jpg', '.png', '.gif'))))\n",
    "\n",
    "    return total_attachments\n",
    "#Used the find_all to find all the tags relates to attachments like: <a>, <img>, <embed>, <iframe>, <object>\n",
    "\n",
    "#3) Check CSS in header (value)\n",
    "def count_css_links(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    link_count = len(soup.find_all(\"link\", rel=\"stylesheet\"))\n",
    "    style_count = len(soup.find_all(\"style\"))\n",
    "    return link_count + style_count\n",
    "\n",
    "#Used the parsed HTML to find all the <link> tag set to rel=stylesheet or <style> \n",
    "\n",
    "\n",
    "#4) Check for external resources (value)\n",
    "def count_external_resources(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return len(soup.find_all(src=True)) + len(soup.find_all(\"link\", href=True))\n",
    "#Find all the src tag or link tag with href to count the number of external resources\n",
    "\n",
    "\n",
    "#5) Check for HTML content (bool)\n",
    "def html_content_str(html):\n",
    "    return bool(re.search(r'html', html, re.IGNORECASE))\n",
    "#r'html' is the expression pattern for the string <html will look into it and re.search will scans through the string html looking for a match\n",
    "#the ignorecase is to make the search case-insensitive\n",
    "\n",
    "#6) Check for HTML form (bool)\n",
    "def html_form(html):\n",
    "    return bool(re.search(r'<\\s?\\/?\\s?form\\s?>', html, re.IGNORECASE))\n",
    "#searching for the form tag <form> by looking for the opening <, \\s?(optional spaces), |/? to allow for </form> and closing angle>\n",
    "\n",
    "#7) Check for iframe Form (bool)\n",
    "def iframe(html):\n",
    "    return bool(re.search(r'<\\s?\\/?\\s?iframe\\s?>', html, re.IGNORECASE))\n",
    "#same type of searching as the form but looking for <iframe> and </iframe>\n",
    "\n",
    "#8) Check for IPs in URLS (bool)\n",
    "def ips_in_urls(urls):\n",
    "    for url in urls:\n",
    "        if re.search(ip_regex, url):\n",
    "            return True\n",
    "    return False\n",
    "#loop into the urls list if found in the url a pattern defined by ip_regenex then return True\n",
    "\n",
    "#9) Check for Javascript Block (value)\n",
    "def count_javascript_blocks(html):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    return len(soup.find_all(\"script\"))\n",
    "#Using the search to find all the <script> tags and counts them\n",
    "\n",
    "#10) Check for URLs in the email\n",
    "def extract_urls_from_html(html):\n",
    "    return re.findall(url_regex, html)\n",
    "    \n",
    "#re.findall searches the entire HTML string to find the pattern: url_regex that contains all the patern for matching URLs\n",
    "#re is tool for defining complex patterns fro string matching (works a plain text) versus beautifulSoup that works as a DOM-like parser that treats HTML as a structured documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5eed116-5502-4cf2-acf8-59f20d8d6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Function to check each conditions in a .txt File\n",
    "\n",
    "def process_file(html):\n",
    "\n",
    "    # Extract URLs from HTML using regex\n",
    "    urls = extract_urls_from_html(html)\n",
    "\n",
    "    # Initialize feature dictionary\n",
    "    feature_dict = {\n",
    "        '@ in URLs': has_at_in_urls(urls),\n",
    "        'Attachments': number_attachments(html),\n",
    "        'CSS': count_css_links(html),\n",
    "        'External Resources': count_external_resources(html),\n",
    "        'HTML Content': html_content_str(html),\n",
    "        'Html Form': html_form(html),\n",
    "        'Html iFrame': iframe(html),\n",
    "        'IPs in URLs': ips_in_urls(urls),\n",
    "        'JavaScript': count_javascript_blocks(html),\n",
    "        'URLs': len(urls)\n",
    "    }\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(columns=feature_dict.keys())  # Create empty DataFrame with columns as feature names\n",
    "    df.loc[0] = feature_dict.values()  # Set the first row with values\n",
    "\n",
    "    # Create an empty DataFrame with feature names as columns and populate the first row with values from the feature dictionary.\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23996c3-25cf-4cb4-83d6-d01d0707f35f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
