{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20686eec-c591-4f60-b357-b4b806589d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6\n",
      "  Downloading torchtext-0.6.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torchtext==0.6) (4.66.5)\n",
      "Requirement already satisfied: requests in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torchtext==0.6) (2.32.3)\n",
      "Requirement already satisfied: torch in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torchtext==0.6) (2.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torchtext==0.6) (1.26.4)\n",
      "Requirement already satisfied: six in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torchtext==0.6) (1.16.0)\n",
      "Collecting sentencepiece (from torchtext==0.6)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from requests->torchtext==0.6) (2024.8.30)\n",
      "Requirement already satisfied: filelock in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from torch->torchtext==0.6) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->torchtext==0.6) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from tqdm->torchtext==0.6) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nickp\\anaconda3\\lib\\site-packages (from jinja2->torch->torchtext==0.6) (2.1.3)\n",
      "Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 992.0/992.0 kB 7.8 MB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "Successfully installed sentencepiece-0.2.0 torchtext-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fa56d0c-9a43-4744-9e2f-79658c06d355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16384 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[0;32m    126\u001b[0m model \u001b[38;5;241m=\u001b[39m RNN(\u001b[38;5;28mlen\u001b[39m(text\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mitos), hidden_units, num_classes)\n\u001b[1;32m--> 127\u001b[0m train(model, train_iter, valid_iter, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[0;32m    129\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[76], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_iter, valid_iter, num_epochs, learning_rate, loss_fn)\u001b[0m\n\u001b[0;32m     94\u001b[0m text_data \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtext[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     95\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mlabel\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 97\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(text_data)\n\u001b[0;32m     98\u001b[0m loss_train \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[76], line 43\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m     42\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m---> 43\u001b[0m     out, _\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(embedded)\n\u001b[0;32m     44\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m     45\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1392\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1392\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1393\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1394\u001b[0m         hx,\n\u001b[0;32m   1395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m   1398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[0;32m   1399\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[0;32m   1400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1401\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[0;32m   1402\u001b[0m     )\n\u001b[0;32m   1403\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1404\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[0;32m   1405\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1406\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1413\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1414\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16384 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchtext\n",
    "# from google.colab import drive, files\n",
    "#Used to import files for Google Colab\n",
    "\n",
    "# drive.mount('/content/gdrive/')\n",
    "\n",
    "text = torchtext.data.Field(sequential=True,\n",
    "                            tokenize=lambda x: x,\n",
    "                            include_lengths=True,\n",
    "                            batch_first=True,\n",
    "                            use_vocab=True)\n",
    "label = torchtext.data.Field(sequential=False,\n",
    "                            use_vocab=False,      \n",
    "                            is_target=True,\n",
    "                            batch_first=True,\n",
    "                            dtype = torch.float)\n",
    "\n",
    "fields = [('text', text), ('label', label)] #Change Below Directory to source training dataset\n",
    "dataset = torchtext.data.TabularDataset(\"/content/gdrive/MyDrive/Colab Notebooks/Spam Ham/spam_ham_datasets.csv\",\"csv\",fields, skip_header=True)\n",
    "train, validate, test = dataset.split(split_ratio=[0.6,0.2,0.2])\n",
    "train_iter = torchtext.data.BucketIterator(train,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False)                  # repeat the iterator for many epochs\n",
    "\n",
    "valid_iter = torchtext.data.BucketIterator(validate,\n",
    "                                           batch_size=32,\n",
    "                                           sort_key=lambda x: len(x.text), # to minimize padding\n",
    "                                           sort_within_batch=True,        # sort within each batch\n",
    "                                           repeat=False)     \n",
    "text.build_vocab(train)\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_units, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_units)\n",
    "        self.rnn = nn.GRU(hidden_units, hidden_units, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, _= self.rnn(embedded)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "def get_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in dataloader:\n",
    "        text_data = batch.text[0]\n",
    "        labels = batch.label.unsqueeze(1).float()  # Ensure labels are [batch_size, 1]\n",
    "        outputs = model(text_data) #make prediction\n",
    "        #compare prediction and add to total number or predictions\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.shape[0]\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    #return accuracy\n",
    "    return correct / total\n",
    "\n",
    "def get_loss(model, dataloader, loss_fn):\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        text_data = batch.text[0]\n",
    "        labels = batch.label.unsqueeze(1).float()\n",
    "        outputs = model(text_data)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        count += 1\n",
    "    return total_loss / count\n",
    "\n",
    "def train(model, train_iter, valid_iter, num_epochs, learning_rate, loss_fn):\n",
    "    torch.manual_seed(26)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    epochs = []\n",
    "    train_acc = []\n",
    "    train_loss = []\n",
    "    val_acc = []\n",
    "    val_loss = []\n",
    "\n",
    "    best_val_acc = 0\n",
    "\n",
    "    print(\"Training started...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epochs.append(epoch + 1)\n",
    "        total_loss_t = 0\n",
    "        count_t = 0\n",
    "        try:\n",
    "            for batch in train_iter:\n",
    "                text_data = batch.text[0]\n",
    "                labels = batch.label.unsqueeze(1).float()\n",
    "                outputs = model(text_data)\n",
    "                loss_train = loss_fn(outputs, labels)\n",
    "                optimizer.zero_grad()\n",
    "                loss_train.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "    \n",
    "                total_loss_t += loss_train.item()\n",
    "                count_t += 1\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing batch: {e}\")\n",
    "\n",
    "        train_acc.append(get_accuracy(model, train_iter))\n",
    "        train_loss.append(total_loss_t / count_t)\n",
    "        val_acc.append(get_accuracy(model, valid_iter))\n",
    "        val_loss.append(get_loss(model, valid_iter, loss_fn))\n",
    "\n",
    "        print(\"epoch: \", epochs[-1], \", train loss: \", train_loss[-1], \", train acc: \", train_acc[-1], \", val loss: \", val_loss[-1], \", val acc: \", val_acc[-1])\n",
    "\n",
    "        # torch.save(model.state_dict(), 'checkpoints/checkpoint.pth')\n",
    "        # files.download('checkpoint.pth')\n",
    "    #Needed to compile and train on Google Colab\n",
    "\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-3\n",
    "hidden_units = 128\n",
    "num_classes = 1\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Instantiate the model\n",
    "model = RNN(len(text.vocab.itos), hidden_units, num_classes)\n",
    "train(model, train_iter, valid_iter, num_epochs=num_epochs, learning_rate=learning_rate, loss_fn=loss_fn)\n",
    "\n",
    "# torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
